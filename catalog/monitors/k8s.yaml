# The official Datadog API documentation with available query parameters & alert types:
# https://docs.datadoghq.com/api/v1/monitors/#create-a-monitor

k8s-deployment-replica-pod-down:
  name: "(k8s) Deployment Replica Pod is down"
  type: query alert
  query: |
    avg(last_15m):avg:kubernetes_state.deployment.replicas_desired{*} by {cluster_name,deployment} - avg:kubernetes_state.deployment.replicas_ready{*} by {cluster_name,deployment} >= 2
  message: |
    ({{cluster_name.name}}) More than one Deployments Replica's pods are down on {{deployment.name}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: true
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 0
  timeout_h: 0
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 5
  threshold_windows: { }
  thresholds:
    critical: 2

k8s-pod-restarting:
  name: "(k8s) Pods are restarting multiple times"
  type: query alert
  query: |
    change(sum(last_5m),last_5m):exclude_null(avg:kubernetes.containers.restarts{*} by {cluster_name,kube_namespace,pod_name}) > 5
  message: |
    ({{cluster_name.name}}) pod {{pod_name.name}} is restarting multiple times on {{kube_namespace.name}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: true
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 0
  timeout_h: 0
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 5
    warning: 3

k8s-statefulset-replica-down:
  name: "(k8s) StatefulSet Replica Pod is down"
  type: query alert
  query: |
    max(last_15m):sum:kubernetes_state.statefulset.replicas_desired{*} by {cluster_name,kube_namespace,statefulset} - sum:kubernetes_state.statefulset.replicas_ready{*} by {cluster_name,kube_namespace,statefulset} >= 2
  message: |
    ({{cluster_name.name}} {{statefulset.name}}) More than one StatefulSet Replica's pods are down on {{kube_namespace.name}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: false
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 0
  timeout_h: 0
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    warning: 1
    critical: 2

k8s-daemonset-pod-down:
  name: "(k8s) DaemonSet Pod is down"
  type: query alert
  query: |
    max(last_15m):sum:kubernetes_state.daemonset.desired{*} by {cluster_name,kube_namespace,daemonset} - sum:kubernetes_state.daemonset.ready{*} by {cluster_name,kube_namespace,daemonset} >= 1
  message: |
    ({{cluster_name.name}} {{daemonset.name}}) One or more DaemonSet pods are down on {{kube_namespace.name}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: false
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 0
  timeout_h: 0
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 1

k8s-crashloopBackOff:
  name: "(k8s) CrashloopBackOff detected"
  type: query alert
  query: |
    max(last_10m):max:kubernetes_state.container.status_report.count.waiting{reason:crashloopbackoff} by {cluster_name,kube_namespace,pod_name} >= 1
  message: |
    ({{cluster_name.name}}) pod {{pod_name.name}} is CrashloopBackOff on {{kube_namespace.name}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: false
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 0
  timeout_h: 0
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 1

k8s-multiple-pods-failing:
  name: "(k8s) Multiple Pods are failing"
  type: query alert
  query: |
    change(avg(last_5m),last_5m):sum:kubernetes_state.pod.status_phase{phase:failed} by {cluster_name,kube_namespace} > 10
  message: |
    ({{cluster_name.name}}) More than ten pods are failing on {{kube_namespace.name}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: true
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 0
  timeout_h: 0
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    warning: 5
    critical: 10

k8s-unavailable-deployment-replica:
  name: "(k8s) Unavailable Deployment Replica(s) detected"
  type: metric alert
  query: |
    max(last_10m):max:kubernetes_state.deployment.replicas_unavailable{*} by {cluster_name,kube_namespace} > 0
  message: |
    ({{cluster_name.name}}) Detected unavailable Deployment replicas for longer than 10 minutes on {{kube_namespace.name}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: false
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 0
    #warning:
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-unavailable-statefulset-replica:
  name: "(k8s) Unavailable Statefulset Replica(s) detected"
  type: metric alert
  query: |
    max(last_10m):max:kubernetes_state.statefulset.replicas_unavailable{*} by {cluster_name,kube_namespace} > 0
  message: |
    ({{cluster_name.name}}) Detected unavailable Statefulset replicas for longer than 10 minutes on {{kube_namespace.name}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: false
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 0
    #warning:
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-node-status-unschedulable:
  name: "(k8s) Detected Unschedulable Node(s)"
  type: query alert
  query: |
    max(last_15m):sum:kubernetes_state.node.status{status:schedulable} by {cluster_name} * 100 / sum:kubernetes_state.node.status{*} by {cluster_name} < 80
  message: |
    More than 20% of nodes are unschedulable on ({{cluster_name}} cluster). \n Keep in mind that this might be expected based on your infrastructure.
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: false
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 80
    warning: 90
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-imagepullbackoff:
  name: "(k8s) ImagePullBackOff detected"
  type: "query alert"
  query: |
    max(last_10m):max:kubernetes_state.container.status_report.count.waiting{reason:imagepullbackoff} by {kube_cluster_name,kube_namespace,pod_name} >= 1
  message: |
    Pod {{pod_name.name}} is ImagePullBackOff on namespace {{kube_namespace.name}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: false
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 1
    #warning:
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-high-cpu-usage:
  name: "(k8s) High CPU Usage Detected"
  type: metric alert
  query: |
    avg(last_10m):avg:system.cpu.system{*} by {host} > 90
  message: |
    ({{host.cluster_name}}) High CPU usage for the last 10 minutes on {{host.name}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: true
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 90
    warning: 60
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-high-disk-usage:
  name: "(k8s) High Disk Usage Detected"
  type: metric alert
  query: |
    min(last_5m):min:system.disk.used{*} by {host,cluster_name} / avg:system.disk.total{*} by {host,cluster_name} * 100 > 90
  message: |
    ({{cluster_name.name}}) High disk usage detected on {{host.name}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: true
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 90
    warning: 75
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-high-memory-usage:
  name: "(k8s) High Memory Usage Detected"
  type: metric alert
  query: |
    avg(last_10m):avg:kubernetes.memory.usage_pct{*} by {cluster_name} > 90
  message: |
    {{#is_warning}}
    {{cluster_name.name}} memory usage greater than 80% for 10 minutes
    {{/is_warning}}
    {{#is_alert}}
    {{cluster_name.name}} memory usage greater than 90% for 10 minutes
    {{/is_alert}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: true
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 90
    warning: 80
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-high-filesystem-usage:
  name: "(k8s) High Filesystem Usage Detected"
  type: metric alert
  query: |
    avg(last_10m):avg:kubernetes.filesystem.usage_pct{*} by {cluster_name} > 90
  message: |
    {{#is_warning}}
    {{cluster_name.name}} filesystem usage greater than 80% for 10 minutes
    {{/is_warning}}
    {{#is_alert}}
    {{cluster_name.name}} filesystem usage greater than 90% for 10 minutes
    {{/is_alert}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: true
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 90
    warning: 80
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-network-tx-errors:
  name: "(k8s) High Network TX (send) Errors"
  type: metric alert
  query: |
    avg(last_10m):avg:kubernetes.network.tx_errors{*} by {cluster_name} > 100
  message: |
    {{#is_warning}}
    {{cluster_name.name}} network TX (send) errors occurring 10 times per second
    {{/is_warning}}
    {{#is_alert}}
    {{cluster_name.name}} network TX (send) errors occurring 100 times per second
    {{/is_alert}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: true
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 100
    warning: 10
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-network-rx-errors:
  name: "(k8s) High Network RX (receive) Errors"
  type: metric alert
  query: |
    avg(last_10m):avg:kubernetes.network.rx_errors{*} by {cluster_name} > 100
  message: |
    {{#is_warning}}
    {{cluster_name.name}} network RX (receive) errors occurring 10 times per second
    {{/is_warning}}
    {{#is_alert}}
    {{cluster_name.name}} network RX (receive) errors occurring 100 times per second
    {{/is_alert}}
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: true
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 100
    warning: 10
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-node-not-ready:
  name: "(k8s) Node Not Ready"
  type: service check
  query: |
    "kubernetes_state.node.ready".by('host').last(5).count_by_status()
  message: |
    ({{host.cluster_name}} {{host.name}} {{host.region}}) Node is reporting 'Not Ready' status
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: false
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 3
    #warning:
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-kube-api-down:
  name: "(k8s) KubeAPI Down"
  type: service check
  query: |
    "kube_apiserver_controlplane.up".by('host').last(5).count_by_status()
  message: |
    ({{cluster_name.name}}) KubeAPI is down
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: true
  require_full_window: false
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 3
    #warning:
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-increased-pod-crash:
  name: "(k8s) Increased Pod Crashes"
  type: query alert
  query: |
    avg(last_5m):avg:kubernetes_state.container.restarts{*} by {cluster_name,kube_namespace,pod} - hour_before(avg:kubernetes_state.container.restarts{*} by {cluster_name,kube_namespace,pod}) > 3
  message: |-
    ({{cluster_name.name}} {{kube_namespace.name}} {{pod.name}}) has crashed repeatedly over the last hour
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: false
  require_full_window: false
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 3
    #warning:
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-hpa-errors:
  ## disabling this, event-v2 follows new structure which needs further testing.
  enabled: false
  name: "(k8s) HPA Errors"
  # must use `event-v2 alert`: https://docs.datadoghq.com/events/guides/migrating_to_new_events_features/
  type: event-v2 alert
  query: |
    events('sources:kubernetes priority:all \"unable to fetch metrics from resource metrics API:\"').by('hpa').rollup('count').last('1h') > 200
  message: |-
    ({{event.tags.cluster_name}}) A high number of hpa failures (> ({{threshold}}) are occurring.
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: false
  require_full_window: true
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 200
    #warning:
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

k8s-pending-pods:
  name: "(k8s) Pending Pods"
  type: metric alert
  query: |
    min(last_30m):sum:kubernetes_state.pod.status_phase{phase:running} by {cluster_name} - sum:kubernetes_state.pod.status_phase{phase:running} by {cluster_name} + sum:kubernetes_state.pod.status_phase{phase:pending} by {cluster_name}.fill(zero) >= 1
  message: |-
    ({{cluster_name.name}}) There has been at least 1 pod Pending for 30 minutes.
    There are currently ({{value}}) pods Pending.
  escalation_message: ""
  tags:
    ManagedBy: Terraform
  priority: 3
  notify_no_data: false
  notify_audit: false
  require_full_window: true
  enable_logs_sample: false
  force_delete: true
  include_tags: true
  locked: false
  renotify_interval: 60
  timeout_h: 60
  evaluation_delay: 60
  new_host_delay: 300
  new_group_delay: 0
  groupby_simple_monitor: false
  renotify_occurrences: 0
  renotify_statuses: []
  validate: true
  no_data_timeframe: 10
  threshold_windows: { }
  thresholds:
    critical: 1
    #warning:
    #unknown:
    #ok:
    #critical_recovery:
    #warning_recovery:

